{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd045819cfb92a21706ccd5013156336b3b4a2ef9690da4bc6190484f558d2f6d31",
   "display_name": "Python 3.7.9 64-bit ('tf': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "45819cfb92a21706ccd5013156336b3b4a2ef9690da4bc6190484f558d2f6d31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import metrics, losses\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/bubble_detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features = scaler.fit_transform(df.iloc[:,1:8])\n",
    "output = np.array(df.iloc[:,9], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 36\n",
    "x = []\n",
    "y = []\n",
    "for row in range(len(features) - time_steps):\n",
    "    x.append(features[row : row + time_steps])\n",
    "    y.append(output[row + time_steps - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x, dtype = 'float64')\n",
    "y = np.array(y, dtype = 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=202144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9460431654676259"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "1 - sum(y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Bidirectional(LSTM(units=50,\n",
    "                    input_shape=(x_train.shape[1], x_train.shape[2]),\n",
    "                    recurrent_activation='sigmoid',\n",
    "                    recurrent_initializer='glorot_uniform')),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "            loss=binary_focal_loss(alpha=0.75),\n",
    "            metrics=[metrics.BinaryAccuracy(), metrics.Precision(), metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 556 samples\n",
      "556/556 [==============================] - 4s 8ms/sample - loss: 0.0343 - binary_accuracy: 0.8921 - precision_13: 0.0312 - recall_13: 0.0333\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc9e8076d90>"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_8\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nbidirectional_7 (Bidirection multiple                  23200     \n_________________________________________________________________\ndense_8 (Dense)              multiple                  101       \n=================================================================\nTotal params: 23,301\nTrainable params: 23,301\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.33441395],\n",
       "       [0.30302417],\n",
       "       [0.3417294 ],\n",
       "       [0.3151436 ],\n",
       "       [0.32966316],\n",
       "       [0.33544472],\n",
       "       [0.31353974],\n",
       "       [0.33672625],\n",
       "       [0.29482037],\n",
       "       [0.32858953],\n",
       "       [0.32807076],\n",
       "       [0.31951374],\n",
       "       [0.32996422],\n",
       "       [0.31622523],\n",
       "       [0.33283246],\n",
       "       [0.3225739 ],\n",
       "       [0.33260208],\n",
       "       [0.32405582],\n",
       "       [0.33006313],\n",
       "       [0.31750435],\n",
       "       [0.31862375],\n",
       "       [0.3272368 ],\n",
       "       [0.31491625],\n",
       "       [0.33387563],\n",
       "       [0.29625887],\n",
       "       [0.3176946 ],\n",
       "       [0.33267063],\n",
       "       [0.30956635],\n",
       "       [0.29439116],\n",
       "       [0.34643626],\n",
       "       [0.2993663 ],\n",
       "       [0.3310492 ],\n",
       "       [0.32779664],\n",
       "       [0.33448997],\n",
       "       [0.3090125 ],\n",
       "       [0.33685058],\n",
       "       [0.33924505],\n",
       "       [0.3554514 ],\n",
       "       [0.33128998],\n",
       "       [0.30531853],\n",
       "       [0.33486265],\n",
       "       [0.3353409 ],\n",
       "       [0.31905496],\n",
       "       [0.3280389 ],\n",
       "       [0.33572602],\n",
       "       [0.3160811 ],\n",
       "       [0.3229942 ],\n",
       "       [0.33184826],\n",
       "       [0.31567106],\n",
       "       [0.31215787],\n",
       "       [0.33053055],\n",
       "       [0.3034697 ],\n",
       "       [0.30438805],\n",
       "       [0.3311461 ],\n",
       "       [0.30515218],\n",
       "       [0.33187425],\n",
       "       [0.32929754],\n",
       "       [0.32829344],\n",
       "       [0.3268143 ],\n",
       "       [0.30411765],\n",
       "       [0.3269775 ],\n",
       "       [0.32201838],\n",
       "       [0.3186763 ],\n",
       "       [0.32336622],\n",
       "       [0.29998016],\n",
       "       [0.32012957],\n",
       "       [0.32634524],\n",
       "       [0.3298704 ],\n",
       "       [0.31789243],\n",
       "       [0.32136652],\n",
       "       [0.32853732],\n",
       "       [0.3224801 ],\n",
       "       [0.32953832],\n",
       "       [0.32371476],\n",
       "       [0.3393546 ],\n",
       "       [0.32930505],\n",
       "       [0.3103268 ],\n",
       "       [0.339065  ],\n",
       "       [0.30544832],\n",
       "       [0.31869447],\n",
       "       [0.3140971 ],\n",
       "       [0.33895147],\n",
       "       [0.33664122],\n",
       "       [0.33058858],\n",
       "       [0.33779007],\n",
       "       [0.3245265 ],\n",
       "       [0.33259696],\n",
       "       [0.29699117],\n",
       "       [0.32601017],\n",
       "       [0.335097  ],\n",
       "       [0.3169236 ],\n",
       "       [0.30530363],\n",
       "       [0.34999713],\n",
       "       [0.3332239 ],\n",
       "       [0.34104586],\n",
       "       [0.3532762 ],\n",
       "       [0.30388147],\n",
       "       [0.31835574],\n",
       "       [0.32030332],\n",
       "       [0.33618405],\n",
       "       [0.33457634],\n",
       "       [0.32735687],\n",
       "       [0.33060795],\n",
       "       [0.31138533],\n",
       "       [0.33580428],\n",
       "       [0.31524786],\n",
       "       [0.32850498],\n",
       "       [0.3016479 ],\n",
       "       [0.32337677],\n",
       "       [0.33764786],\n",
       "       [0.3339467 ],\n",
       "       [0.3106306 ],\n",
       "       [0.3121335 ],\n",
       "       [0.33623278],\n",
       "       [0.32352558],\n",
       "       [0.33551943],\n",
       "       [0.3286925 ],\n",
       "       [0.31751597],\n",
       "       [0.32538635],\n",
       "       [0.3292661 ],\n",
       "       [0.3186825 ],\n",
       "       [0.3326191 ],\n",
       "       [0.33115947],\n",
       "       [0.31868976],\n",
       "       [0.31957138],\n",
       "       [0.31957364],\n",
       "       [0.32122257],\n",
       "       [0.34199187],\n",
       "       [0.33403212],\n",
       "       [0.31893748],\n",
       "       [0.33365616],\n",
       "       [0.31640652],\n",
       "       [0.3239808 ],\n",
       "       [0.33755872],\n",
       "       [0.32895848],\n",
       "       [0.33708286],\n",
       "       [0.33556443],\n",
       "       [0.32454193],\n",
       "       [0.32589298],\n",
       "       [0.32216805]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[134,   0],\n",
       "       [  6,   0]])"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from: https://github.com/aldi-dimara/keras-focal-loss/blob/master/focal_loss.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Oct 19 08:20:58 2018\n",
    "@OS: Ubuntu 18.04\n",
    "@IDE: Spyder3\n",
    "@author: Aldi Faizal Dimara (Steam ID: phenomos)\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def binary_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss from the paper in multiclass classification\n",
    "    Formula:\n",
    "        loss = -alpha_t*((1-p_t)^gamma)*log(p_t)\n",
    "        \n",
    "        p_t = y_pred, if y_true = 1\n",
    "        p_t = 1-y_pred, otherwise\n",
    "        \n",
    "        alpha_t = alpha, if y_true=1\n",
    "        alpha_t = 1-alpha, otherwise\n",
    "        \n",
    "        cross_entropy = -log(p_t)\n",
    "    Parameters:\n",
    "        alpha -- the same as wighting factor in balanced cross entropy\n",
    "        gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "        gamma -- 2.0 as mentioned in the paper\n",
    "        alpha -- 0.25 as mentioned in the paper\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Define epsilon so that the backpropagation will not result in NaN\n",
    "        # for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        #y_pred = y_pred + epsilon\n",
    "        # Clip the prediciton value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1-y_pred)\n",
    "        # Calculate alpha_t\n",
    "        alpha_factor = K.ones_like(y_true)*alpha\n",
    "        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1-alpha_factor)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -K.log(p_t)\n",
    "        weight = alpha_t * K.pow((1-p_t), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.sum(loss, axis=1)\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss"
   ]
  }
 ]
}