{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd045819cfb92a21706ccd5013156336b3b4a2ef9690da4bc6190484f558d2f6d31",
   "display_name": "Python 3.7.9 64-bit ('tf': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "45819cfb92a21706ccd5013156336b3b4a2ef9690da4bc6190484f558d2f6d31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import metrics, losses\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/bubble_detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features = scaler.fit_transform(df.iloc[:,1:-1])\n",
    "output = np.array(df.iloc[:,-1], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 36\n",
    "x = []\n",
    "y = []\n",
    "for row in range(len(features) - time_steps):\n",
    "    x.append(features[row : row + time_steps])\n",
    "    y.append(output[row + time_steps - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x, dtype = 'float64')\n",
    "y = np.array(y, dtype = 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=202144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9460431654676259"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "source": [
    "1 - sum(y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Bidirectional(LSTM(units=50,\n",
    "                    input_shape=(x_train.shape[1], x_train.shape[2]),\n",
    "                    recurrent_activation='sigmoid',\n",
    "                    recurrent_initializer='glorot_uniform')),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "            loss=binary_focal_loss(alpha=0.75),\n",
    "            metrics=[metrics.BinaryAccuracy(), metrics.Precision(), metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 556 samples\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0350 - binary_accuracy: 0.9442 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fca0cb193d0>"
      ]
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nbidirectional_9 (Bidirection multiple                  25200     \n_________________________________________________________________\ndense_10 (Dense)             multiple                  101       \n=================================================================\nTotal params: 25,301\nTrainable params: 25,301\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.37766057],\n",
       "       [0.33260363],\n",
       "       [0.37189746],\n",
       "       [0.35877505],\n",
       "       [0.3638599 ],\n",
       "       [0.37044907],\n",
       "       [0.3518231 ],\n",
       "       [0.3715133 ],\n",
       "       [0.3398953 ],\n",
       "       [0.36495477],\n",
       "       [0.36285537],\n",
       "       [0.34865397],\n",
       "       [0.36037254],\n",
       "       [0.34839475],\n",
       "       [0.37029207],\n",
       "       [0.3518267 ],\n",
       "       [0.34826645],\n",
       "       [0.36696815],\n",
       "       [0.35709763],\n",
       "       [0.3407362 ],\n",
       "       [0.36130857],\n",
       "       [0.36293617],\n",
       "       [0.34074622],\n",
       "       [0.3493533 ],\n",
       "       [0.3359724 ],\n",
       "       [0.34984785],\n",
       "       [0.36220753],\n",
       "       [0.34523058],\n",
       "       [0.32218894],\n",
       "       [0.39254412],\n",
       "       [0.33526725],\n",
       "       [0.3589903 ],\n",
       "       [0.36786404],\n",
       "       [0.37393075],\n",
       "       [0.33414018],\n",
       "       [0.37377554],\n",
       "       [0.37522924],\n",
       "       [0.38861397],\n",
       "       [0.366619  ],\n",
       "       [0.33519566],\n",
       "       [0.36803883],\n",
       "       [0.35374326],\n",
       "       [0.34463346],\n",
       "       [0.3626184 ],\n",
       "       [0.37787366],\n",
       "       [0.3568322 ],\n",
       "       [0.3629298 ],\n",
       "       [0.37575758],\n",
       "       [0.354083  ],\n",
       "       [0.3349835 ],\n",
       "       [0.36736012],\n",
       "       [0.3372231 ],\n",
       "       [0.3339665 ],\n",
       "       [0.37546045],\n",
       "       [0.3361187 ],\n",
       "       [0.35621706],\n",
       "       [0.3607967 ],\n",
       "       [0.35888332],\n",
       "       [0.36646992],\n",
       "       [0.33097947],\n",
       "       [0.3587662 ],\n",
       "       [0.34719318],\n",
       "       [0.35298115],\n",
       "       [0.35060745],\n",
       "       [0.33867866],\n",
       "       [0.3633986 ],\n",
       "       [0.36195183],\n",
       "       [0.36945385],\n",
       "       [0.35166332],\n",
       "       [0.34526792],\n",
       "       [0.35218573],\n",
       "       [0.35201648],\n",
       "       [0.3612048 ],\n",
       "       [0.35268575],\n",
       "       [0.37461695],\n",
       "       [0.34569114],\n",
       "       [0.34013212],\n",
       "       [0.35389805],\n",
       "       [0.34254697],\n",
       "       [0.34560058],\n",
       "       [0.35671955],\n",
       "       [0.3766479 ],\n",
       "       [0.36786234],\n",
       "       [0.3559573 ],\n",
       "       [0.37647915],\n",
       "       [0.34209204],\n",
       "       [0.37655327],\n",
       "       [0.32933623],\n",
       "       [0.36417764],\n",
       "       [0.37316138],\n",
       "       [0.3488925 ],\n",
       "       [0.34449548],\n",
       "       [0.39820716],\n",
       "       [0.36085635],\n",
       "       [0.37638146],\n",
       "       [0.3887664 ],\n",
       "       [0.33693808],\n",
       "       [0.34867397],\n",
       "       [0.34196916],\n",
       "       [0.37567723],\n",
       "       [0.37522715],\n",
       "       [0.35894075],\n",
       "       [0.37361473],\n",
       "       [0.33663344],\n",
       "       [0.37329087],\n",
       "       [0.34917426],\n",
       "       [0.36332408],\n",
       "       [0.35138208],\n",
       "       [0.35198927],\n",
       "       [0.37855375],\n",
       "       [0.37408823],\n",
       "       [0.33414066],\n",
       "       [0.34255725],\n",
       "       [0.37395763],\n",
       "       [0.35124677],\n",
       "       [0.36717755],\n",
       "       [0.35908753],\n",
       "       [0.35784438],\n",
       "       [0.35612762],\n",
       "       [0.3598585 ],\n",
       "       [0.35993993],\n",
       "       [0.37565088],\n",
       "       [0.35675994],\n",
       "       [0.34612587],\n",
       "       [0.34442776],\n",
       "       [0.34874883],\n",
       "       [0.35520267],\n",
       "       [0.38573444],\n",
       "       [0.36723313],\n",
       "       [0.3510564 ],\n",
       "       [0.37379554],\n",
       "       [0.34293348],\n",
       "       [0.3505879 ],\n",
       "       [0.3782461 ],\n",
       "       [0.3595341 ],\n",
       "       [0.37046313],\n",
       "       [0.3782757 ],\n",
       "       [0.36631972],\n",
       "       [0.3488426 ],\n",
       "       [0.3479702 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[134,   0],\n",
       "       [  6,   0]])"
      ]
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from: https://github.com/aldi-dimara/keras-focal-loss/blob/master/focal_loss.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Oct 19 08:20:58 2018\n",
    "@OS: Ubuntu 18.04\n",
    "@IDE: Spyder3\n",
    "@author: Aldi Faizal Dimara (Steam ID: phenomos)\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def binary_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss from the paper in multiclass classification\n",
    "    Formula:\n",
    "        loss = -alpha_t*((1-p_t)^gamma)*log(p_t)\n",
    "        \n",
    "        p_t = y_pred, if y_true = 1\n",
    "        p_t = 1-y_pred, otherwise\n",
    "        \n",
    "        alpha_t = alpha, if y_true=1\n",
    "        alpha_t = 1-alpha, otherwise\n",
    "        \n",
    "        cross_entropy = -log(p_t)\n",
    "    Parameters:\n",
    "        alpha -- the same as wighting factor in balanced cross entropy\n",
    "        gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "        gamma -- 2.0 as mentioned in the paper\n",
    "        alpha -- 0.25 as mentioned in the paper\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Define epsilon so that the backpropagation will not result in NaN\n",
    "        # for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        #y_pred = y_pred + epsilon\n",
    "        # Clip the prediciton value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1-y_pred)\n",
    "        # Calculate alpha_t\n",
    "        alpha_factor = K.ones_like(y_true)*alpha\n",
    "        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1-alpha_factor)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -K.log(p_t)\n",
    "        weight = alpha_t * K.pow((1-p_t), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.sum(loss, axis=1)\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss"
   ]
  }
 ]
}